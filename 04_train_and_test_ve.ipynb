{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Please visit:\n",
    "#\n",
    "# https://download.nkg-mn.com/credo/anomalies/\n",
    "#\n",
    "# Pretrained dots_100000.h5 and others can be downloaded from mentioned website."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from pyimagesearch.vae import build_vae_autoencoder, train_vae\n",
    "from commons import *\n",
    "from dataset_loader import load_from_file\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "DOTS_DST = 'cache/dots_100000.pickle'\n",
    "TRACKS_DST = 'cache/tracks_100000.pickle'\n",
    "WORMS_DST = 'cache/worms_100000.pickle'\n",
    "ARTIFACTS_DST = 'cache/artifacts_100000.pickle'\n",
    "\n",
    "DOTS_AC = 'cache/dots_vae_100000.tf'\n",
    "TRACKS_AC = 'cache/tracks_vae_100000.tf'\n",
    "WORMS_AC = 'cache/worms_vae_100000.tf'\n",
    "ARTIFACTS_AC = 'cache/artifacts_vae_100000.tf'\n",
    "\n",
    "\n",
    "def train_and_test(train_set, validation_set, fn):\n",
    "    _, __, autoencoder = build_vae_autoencoder()\n",
    "    autoencoder = train_vae(autoencoder, fn, train_set=train_set, validation_set=validation_set)\n",
    "\n",
    "    for df_func, df_name in zip(\n",
    "            [dm_func_mean],\n",
    "            ['mean']\n",
    "    ):\n",
    "        for img_set, set_names in zip([validation_set], ['train']):\n",
    "            decoded = autoencoder.predict(img_set)\n",
    "\n",
    "            vis, errors = visualize_predictions(decoded, img_set, df_func, False, 16)\n",
    "            img_path = fn.replace('.h5', '-vis.png')\n",
    "            hist_path = fn.replace('.h5', '-hist.png')\n",
    "            cv2.imwrite(img_path, vis)\n",
    "            display(Image.open(img_path))\n",
    "            if len(img_set) > 256:\n",
    "                vis, errors = visualize_predictions(decoded, img_set, df_func, False)\n",
    "                img_path = fn.replace('.h5', '-vis_full.png')\n",
    "                cv2.imwrite(img_path, vis)\n",
    "\n",
    "            plt.figure(figsize=(8,6))\n",
    "            plt.hist(errors, bins=100, alpha=0.5, label=set_names)\n",
    "            plt.xlabel(\"Data\", size=14)\n",
    "            plt.ylabel(\"Count\", size=14)\n",
    "            plt.title(df_name)\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.savefig(hist_path)\n",
    "\n",
    "\n",
    "def load_and_train(src, out):\n",
    "    images, augmented = load_from_file(src)\n",
    "    train_and_test(np.expand_dims(augmented, axis=-1), np.expand_dims(images, axis=-1), out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 14, 14, 32)   320         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 7, 7, 64)     18496       ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 3136)         0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           50192       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 2)            34          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 2)            34          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 2)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 69,076\n",
      "Trainable params: 69,076\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3136)              9408      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 14, 14, 64)       36928     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 28, 28, 32)       18464     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 28, 28, 1)        289       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,089\n",
      "Trainable params: 65,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Specified input shape is not one of the valid types. Please specify a batch input shape of type tuple or list of input shapes. User provided input type: <class 'numpy.ndarray'>.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [3]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mload_and_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDOTS_DST\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mDOTS_AC\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36mload_and_train\u001B[1;34m(src, out)\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_and_train\u001B[39m(src, out):\n\u001B[0;32m     50\u001B[0m     images, augmented \u001B[38;5;241m=\u001B[39m load_from_file(src)\n\u001B[1;32m---> 51\u001B[0m     \u001B[43mtrain_and_test\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexpand_dims\u001B[49m\u001B[43m(\u001B[49m\u001B[43maugmented\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexpand_dims\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36mtrain_and_test\u001B[1;34m(train_set, validation_set, fn)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_and_test\u001B[39m(train_set, validation_set, fn):\n\u001B[0;32m     20\u001B[0m     _, __, autoencoder \u001B[38;5;241m=\u001B[39m build_vae_autoencoder()\n\u001B[1;32m---> 21\u001B[0m     autoencoder \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_vae\u001B[49m\u001B[43m(\u001B[49m\u001B[43mautoencoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_set\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_set\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_set\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_set\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m df_func, df_name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\n\u001B[0;32m     24\u001B[0m             [dm_func_mean],\n\u001B[0;32m     25\u001B[0m             [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmean\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     26\u001B[0m     ):\n\u001B[0;32m     27\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m img_set, set_names \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m([validation_set], [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m]):\n",
      "File \u001B[1;32m~\\source\\repos\\auto_encoder\\pyimagesearch\\vae.py:121\u001B[0m, in \u001B[0;36mtrain_vae\u001B[1;34m(vae, fn, train_set, validation_set, epochs)\u001B[0m\n\u001B[0;32m    119\u001B[0m validation_set \u001B[38;5;241m=\u001B[39m as_mnist(validation_set)\n\u001B[0;32m    120\u001B[0m \u001B[38;5;66;03m#data = np.expand_dims(data, -1)\u001B[39;00m\n\u001B[1;32m--> 121\u001B[0m \u001B[43mvae\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    123\u001B[0m H \u001B[38;5;241m=\u001B[39m vae\u001B[38;5;241m.\u001B[39mfit(\n\u001B[0;32m    124\u001B[0m     data,\n\u001B[0;32m    125\u001B[0m     epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m    126\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m128\u001B[39m,\n\u001B[0;32m    127\u001B[0m     \u001B[38;5;66;03m#validation_data=(validation_set, validation_set) if validation_set is not None else None,\u001B[39;00m\n\u001B[0;32m    128\u001B[0m )\n\u001B[0;32m    130\u001B[0m \u001B[38;5;28mprint\u001B[39m(vae\u001B[38;5;241m.\u001B[39msave_spec() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[1;32mc:\\users\\nkg\\source\\repos\\auto_encoder\\venv\\lib\\site-packages\\keras\\engine\\training.py:381\u001B[0m, in \u001B[0;36mModel.build\u001B[1;34m(self, input_shape)\u001B[0m\n\u001B[0;32m    379\u001B[0m valid_types \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mtuple\u001B[39m, \u001B[38;5;28mlist\u001B[39m, tf\u001B[38;5;241m.\u001B[39mTensorShape, \u001B[38;5;28mdict\u001B[39m)\n\u001B[0;32m    380\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(input_shape, valid_types):\n\u001B[1;32m--> 381\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSpecified input shape is not one of the valid types. \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    382\u001B[0m                    \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPlease specify a batch input shape of type tuple or \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    383\u001B[0m                    \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlist of input shapes. User provided \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    384\u001B[0m                    \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput type: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mtype\u001B[39m(input_shape)))\n\u001B[0;32m    386\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m input_shape \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minputs:\n\u001B[0;32m    387\u001B[0m   \u001B[38;5;66;03m# We create placeholders for the `None`s in the shape and build the model\u001B[39;00m\n\u001B[0;32m    388\u001B[0m   \u001B[38;5;66;03m# in a Graph. Since tf.Variable is compatible with both eager execution\u001B[39;00m\n\u001B[0;32m    389\u001B[0m   \u001B[38;5;66;03m# and graph building, the variables created after building the model in\u001B[39;00m\n\u001B[0;32m    390\u001B[0m   \u001B[38;5;66;03m# a Graph are still valid when executing eagerly.\u001B[39;00m\n\u001B[0;32m    391\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mexecuting_eagerly():\n",
      "\u001B[1;31mValueError\u001B[0m: Specified input shape is not one of the valid types. Please specify a batch input shape of type tuple or list of input shapes. User provided input type: <class 'numpy.ndarray'>."
     ]
    }
   ],
   "source": [
    "load_and_train(DOTS_DST, DOTS_AC)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "load_and_train(TRACKS_DST, TRACKS_AC)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "load_and_train(WORMS_DST, WORMS_AC)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "load_and_train(ARTIFACTS_DST, ARTIFACTS_AC)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}